{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff7e5bf-fcf0-4f12-8254-5e86082d4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d027f8c-8299-4c9d-a084-b9bca36cb374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = SparkContext(\"local[2]\", \"DStreamExperimental\")\n",
    "# ssc = streamingContext(sc, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bafe6e6-2b31-4ef4-90ac-09edf11c01e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c8615-2ecb-4d55-871d-c61530697280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create SparkContext and StreamingContext with batch interval of 5 seconds\n",
    "sc = SparkContext.getOrCreate()\n",
    "ssc = StreamingContext(sc, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c45e1b0-ca66-4abd-82e6-5910be4dbfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create an RDD queue with at least one empty RDD to avoid deque errors\n",
    "rdd_queue = deque([sc.emptyRDD()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee41dc1-c440-4332-9a7e-29333c4c3bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a DStream from the RDD queue\n",
    "input_stream = ssc.queueStream(rdd_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c44757-bbb4-4736-a36c-60f8b8500388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Perform word count\n",
    "words = input_stream.flatMap(lambda line: line.split(\" \"))\n",
    "word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa24300c-a666-40ab-8c9c-401d63dc4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Print the results\n",
    "word_counts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb3067-e947-412b-be03-55f13183f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Read the file and add it as an RDD to the queue\n",
    "file_path = \"windows11_is_trash.txt\"\n",
    "file_rdd = sc.textFile(file_path)\n",
    "rdd_queue.append(file_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8632f109-a55d-4cff-a635-52787a2ae2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Start streaming\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe02a30-eaf8-46d7-9ef1-d86d38606910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "windows 11 is a great OS if and only if you have never used windows 7 or windows 10. seriously, even after having the best hardware that is currently offered by the current generation that is the Raptor Lake Refresh HX series with 24 cores. the system still feels choppy and running IntelliJ causes Lag spikes as the CPU starts to boost two cores using Intel Enhanced Thermal Velocity Boost. since usually in idle cases only a single core is active and the others are parked it takes a small time to unpark the next Raptor Cove P core to keep up with the OS. How did we get here?\n"
     ]
    }
   ],
   "source": [
    "file = open(\"windows11_is_trash.txt\")\n",
    "content = file.read()\n",
    "print(content)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673bb7d9-7acf-49c9-b1c3-7a81d6122a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.streaming.dstream.TransformedDStream object at 0x7fae19c15550>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# Monitor directory for new files\n",
    "input_stream = ssc.textFileStream(\"windows11_is_trash.txt\")\n",
    "\n",
    "words = input_stream.flatMap(lambda line: line.split(\" \"))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c35efbf0-2b89-4eda-b8b4-3a95ef9b4c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.streaming.dstream.TransformedDStream object at 0x7fae19b180d0>\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.streaming.api.python.PythonTransformedDStream.\n: java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after starting a context is not supported\n\tat org.apache.spark.streaming.dstream.DStream.validateAtInit(DStream.scala:226)\n\tat org.apache.spark.streaming.dstream.DStream.<init>(DStream.scala:67)\n\tat org.apache.spark.streaming.api.python.PythonDStream.<init>(PythonDStream.scala:224)\n\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.<init>(PythonDStream.scala:241)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results:  \u001b[38;5;66;03m# Avoid printing empty results\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord Counts:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mword_counts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeachRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprint_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m ssc\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/streaming/dstream.py:241\u001b[0m, in \u001b[0;36mDStream.foreachRDD\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    240\u001b[0m api \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonDStream\n\u001b[0;32m--> 241\u001b[0m api\u001b[38;5;241m.\u001b[39mcallForeachRDD(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdstream\u001b[49m, jfunc)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/streaming/dstream.py:931\u001b[0m, in \u001b[0;36mTransformedDStream._jdstream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    929\u001b[0m jfunc \u001b[38;5;241m=\u001b[39m TransformFunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev\u001b[38;5;241m.\u001b[39m_jrdd_deserializer)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 931\u001b[0m dstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonTransformedDStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprev\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdstream_val \u001b[38;5;241m=\u001b[39m dstream\u001b[38;5;241m.\u001b[39masJavaDStream()\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdstream_val\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1579\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1581\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1584\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1585\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1589\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.streaming.api.python.PythonTransformedDStream.\n: java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after starting a context is not supported\n\tat org.apache.spark.streaming.dstream.DStream.validateAtInit(DStream.scala:226)\n\tat org.apache.spark.streaming.dstream.DStream.<init>(DStream.scala:67)\n\tat org.apache.spark.streaming.api.python.PythonDStream.<init>(PythonDStream.scala:224)\n\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.<init>(PythonDStream.scala:241)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(word_counts)\n",
    "\n",
    "def print_results(rdd):\n",
    "    results = rdd.collect()\n",
    "    if results:  # Avoid printing empty results\n",
    "        print(\"Word Counts:\", results)\n",
    "\n",
    "word_counts.foreachRDD(print_results)\n",
    "\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c1f68-822b-4b1c-a624-d3e1d6eaa145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
