{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78bca7ff-2507-4b07-8f94-bed79547b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, DoubleType, StringType, IntegerType, StructField\n",
    "from pyspark.sql.functions import col, split, trim\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import StructType, DoubleType, StringType, IntegerType, StructField\n",
    "from pyspark.sql.functions import col, split, trim\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb3ad465-1cb6-45fe-b444-4cd3b55dfab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToElasticsearchHwinfoLogs\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2,org.elasticsearch:elasticsearch-spark-30_2.12:8.10.1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa38a85-878b-4482-b4d7-8c8848761397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://6b0816de4193:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>KafkaToElasticsearchHwinfoLogs</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f597921f5e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06024619-eb89-4a6c-96cc-0f38d271ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kafka = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"hwinfo\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b46245df-de02-43a4-b7b7-1918d1ecec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "hwinfo_schema = StructType([\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"Time\", StringType(), True),\n",
    "    StructField(\"Core_VIDs_avg_V\", DoubleType(), True),\n",
    "    StructField(\"Core_Clocks_avg_MHz\", IntegerType(), True),\n",
    "    StructField(\"Ring_LLC_Clock_MHz\", DoubleType(), True),\n",
    "    StructField(\"Core_Usage_avg_percent\", DoubleType(), True),\n",
    "    StructField(\"Core_Temperatures_avg_C\", DoubleType(), True),\n",
    "    StructField(\"Core_Distance_to_TjMAX_avg_C\", DoubleType(), True),\n",
    "    StructField(\"CPU_Package_C\", IntegerType(), True),\n",
    "    StructField(\"CPU_Package_Power_W\", DoubleType(), True),\n",
    "    StructField(\"PL1_Power_Limit_Static_W\", DoubleType(), True),\n",
    "    StructField(\"PL1_Power_Limit_Dynamic_W\", DoubleType(), True),\n",
    "    StructField(\"PL2_Power_Limit_Static_W\", DoubleType(), True),\n",
    "    StructField(\"PL2_Power_Limit_Dynamic_W\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25abe702-2350-4f1a-9f17-829868ec442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the binary shit show to what the it should actually represent\n",
    "json_data = df_kafka.selectExpr(\"CAST(value AS STRING)\").alias(\"strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50db8ed7-60cf-47f4-90d4-a06b74742126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed = json_data.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "df_final = df_parsed.withColumn(\"value\", regexp_replace(col(\"value\"), \"[\\\\[\\\\]']\", \"\")) \n",
    "\n",
    "columns = [\n",
    "    \"Date\", \"Time\", \"Core_VIDs_avg_V\", \"Core_Clocks_avg_MHz\", \"Ring_LLC_Clock_MHz\",\n",
    "    \"Core_Usage_avg_percent\", \"Core_Temperatures_avg_C\", \"Core_Distance_to_TjMAX_avg_C\",\n",
    "    \"CPU_Package_C\", \"CPU_Package_Power_W\", \"PL1_Power_Limit_Static_W\",\n",
    "    \"PL1_Power_Limit_Dynamic_W\", \"PL2_Power_Limit_Static_W\", \"PL2_Power_Limit_Dynamic_W\"\n",
    "]\n",
    "\n",
    "df_split = df_final.withColumn(\"value\", split(col(\"value\"), \",\"))\n",
    "\n",
    "for i, column in enumerate(columns):\n",
    "    df_split = df_split.withColumn(column, df_split[\"value\"].getItem(i))\n",
    "\n",
    "df_cleaned = df_split.select(\n",
    "    col(\"Date\"), col(\"Time\"),\n",
    "    col(\"Core_VIDs_avg_V\").cast(\"double\"),\n",
    "    col(\"Core_Clocks_avg_MHz\").cast(\"int\"),\n",
    "    col(\"Ring_LLC_Clock_MHz\").cast(\"double\"),\n",
    "    col(\"Core_Usage_avg_percent\").cast(\"double\"),\n",
    "    col(\"Core_Temperatures_avg_C\").cast(\"double\"),\n",
    "    col(\"Core_Distance_to_TjMAX_avg_C\").cast(\"double\"),\n",
    "    col(\"CPU_Package_C\").cast(\"int\"),\n",
    "    col(\"CPU_Package_Power_W\").cast(\"double\"),\n",
    "    col(\"PL1_Power_Limit_Static_W\").cast(\"double\"),\n",
    "    col(\"PL1_Power_Limit_Dynamic_W\").cast(\"double\"),\n",
    "    col(\"PL2_Power_Limit_Static_W\").cast(\"double\"),\n",
    "    col(\"PL2_Power_Limit_Dynamic_W\").cast(\"double\")\n",
    ")\n",
    "# df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30ee943d-8f81-4913-a302-9fcb581ee809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the damn model\n",
    "model = PipelineModel.load(\"model1\")\n",
    "\n",
    "feature_columns = [\n",
    "    \"Core_VIDs_avg_V\", \"Core_Clocks_avg_MHz\", \"Ring_LLC_Clock_MHz\", \"Core_Usage_avg_percent\",\n",
    "    \"Core_Temperatures_avg_C\", \"Core_Distance_to_TjMAX_avg_C\", \"CPU_Package_C\",\n",
    "    \"CPU_Package_Power_W\", \"PL1_Power_Limit_Static_W\", \"PL1_Power_Limit_Dynamic_W\",\n",
    "    \"PL2_Power_Limit_Static_W\", \"PL2_Power_Limit_Dynamic_W\"\n",
    "]\n",
    "\n",
    "# df_predictions = model.transform(df_cleaned) \\\n",
    "#     .select(\"Date\", \"Time\", \"prediction\") \\\n",
    "#     .withColumnRenamed(\"prediction\", \"Thermal_Throttling\")  # Rename to meaningful column name\n",
    "\n",
    "df_final_with_prediction = model.transform(df_cleaned) \\\n",
    "    .withColumnRenamed(\"prediction\", \"Core_Thermal_Throttling\") \\\n",
    "    .select(\"Date\", \"Time\", \"Core_VIDs_avg_V\", \"Core_Clocks_avg_MHz\", \"Ring_LLC_Clock_MHz\", \"Core_Usage_avg_percent\", \"Core_Temperatures_avg_C\", \"Core_Distance_to_TjMAX_avg_C\", \"CPU_Package_C\", \"CPU_Package_Power_W\", \"PL1_Power_Limit_Static_W\", \"PL1_Power_Limit_Dynamic_W\", \"PL2_Power_Limit_Static_W\", \"PL2_Power_Limit_Dynamic_W\", \"Core_Thermal_Throttling\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af9c2047-565f-4162-8964-7a39726cab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating two streaming from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f50cd3b-2dd3-4f68-8edf-0b57c361c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream sink 1 : going directly to elasticsearch for future querying and other random nonsense\n",
    "# use resource = \"hwinfo\" for production\n",
    "# use resource = \"hwinfo_test\" for testing\n",
    "\n",
    "es_query = df_final_with_prediction.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"es.nodes\", \"elasticsearch\") \\\n",
    "    .option(\"es.port\", \"9200\") \\\n",
    "    .option(\"es.resource\", \"hwinfo_test\") \\\n",
    "    .option(\"es.net.ssl\", \"false\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/spark-checkpoints\") \\\n",
    "    .start()\n",
    "\n",
    "# # stream sink 2: used for serving the real time dashboard, stream to a kafka topic that will do all the necessary shit\n",
    "kafka_query = df_final_with_prediction.selectExpr(\"to_json(struct(*)) AS value\") \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"hwinfo_logs_RT\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/spark-checkpoints-RT\") \\\n",
    "    .start()\n",
    "\n",
    "# under testing \n",
    "\n",
    "# Write messages to console\n",
    "# query = df_final.writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()\n",
    "\n",
    "\n",
    "# stream sink 2: used for serving the real time dashboard, stream to a kafka topic that will do all the necessary shit\n",
    "# kafka_query = df_final.writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "#     .option(\"topic\", \"hwinfo_logs_RT\") \\\n",
    "#     .option(\"checkpointLocation\", \"/tmp/spark-checkpoints-RT\") \\\n",
    "#     .start()\n",
    "\n",
    "# the above code should error out since kafka needs value as a message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4e2e9b2-4ed6-4e71-b607-254c3e58450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_query.stop()\n",
    "kafka_query.stop()\n",
    "\n",
    "# es_query = df_final.writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"org.elasticsearch.spark.sql\") \\\n",
    "#     .option(\"es.nodes\", \"elasticsearch\") \\\n",
    "#     .option(\"es.port\", \"9200\") \\\n",
    "#     .option(\"es.resource\", \"hwinfo\") \\\n",
    "#     .option(\"es.net.ssl\", \"false\") \\\n",
    "#     .option(\"checkpointLocation\", \"/tmp/spark-checkpoints\") \\\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41dd8a2-15dc-4a29-a98f-2fce9b8f264e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
